# Specs

*Note: This feature is currently being built and thus not yet implemented.*

Lore supports built-in testing and benchmarking via **specs**. They are first-class entities, which enables programmers to quickly add tests without having to resort to a third-party library. Being first-class promotes a close relationship between functions and their tests and benchmarks, as functions and specs can live close together in the same fragment. Specs are also included in the bytecode generated by the Lore compiler, which allows a Lore program to be verified and benchmarked even when it has already been shipped.



### Spec Declarations

A **spec** is declared with the `spec` keyword, an optional name, and a body. The name may be an identifier or a string. Specs may share names with types and bindings, but not with other specs. The body usually contains assertions. If no assertions fail during the execution of the spec, the execution is successful. Importing or otherwise referring to specs within Lore code is impossible, as they must be executed using the VM's `test` and `bench` commands.

By default, a spec will only be executed as a test. A spec can optionally be marked as a `@bench` to include it in the suite of benchmarks. If the spec makes no contribution to testing, which is the case for pure benchmarks, the spec may be marked as `@bench_only` to include it in the suite of benchmarks, but exclude it from the suite of tests. 

Specs are part of regular fragments and may be declared anywhere at the top level. Sometimes, a spec will be declared right beside the function it tests, and other times in a separate fragment such as `list.spec.lore`. It depends on the situation which approach is preferable.

###### Syntax Example

```
spec name do
  // body
end

spec 'a string name' do
  // body
end

spec do
  // body
end

@bench
spec name2 do
  // body
end

@bench_only
spec name3 do
  // body
end
```

###### Example

```
module lore.number

use lore.test._

spec min_max do
  assert_equal(min(-1, 1), -1)
  assert_equal(max(-1, 1), 1)
end

spec 'min should return the smaller integer' do
  assert_equal(min(4, 8), 4)
end 

@bench_only
spec bench_min_max do
  max(min(10, 4), min(max(5, 2), 7))
end
```



### Testing and Benchmarking

Specs can be executed directly by the VM as **tests** or as **benchmarks**. The VM provides the respective commands for this: `test` and `bench`. Both commands allow filtering specs by name paths, which are either fully qualified spec names or module names with a wildcard. Only specs that are matched by one of the specified name paths will be included. When no name paths are given, all specs will be executed.

Regardless of the name paths specified, a spec that isn't marked as a test/benchmark will not be executed as such. Directly referenced specs must be executable for the given command, or the execution results in an error. For example, `lore.number.min_max` is not a benchmark, so running `bench lore.number.min_max` will result in an error: "The spec `lore.number.min_max` is not a benchmark."

Spec name paths follow the following standard:

- **Spec name:** `module1.module2.spec_name` (e.g. `lore.number.min_max`)
  - Spec names that contain spaces or other characters incompatible with the shell cannot be used as a filter. In such cases, a module filter should be used.
- **Module name:** `module1.module2._` (e.g. `lore.number._`)
  - The `_` acts as a wildcard and includes all specs in the specified module, as well as all specs declared in any of the module's submodules.

The `test` command executes all desired specs and for each spec, reports its name, execution time, and whether the test was successful or not. If unsuccessful, the error message of the failed assertion will be reported as well. In the future, failed tests will be accompanied by a stack trace. This is currently not possible, however, as the VM does not support stack traces yet.

The `bench` command works similarly. Each benchmark spec is executed in a warmup phase for 2 seconds, during which the number of iterations per second is recorded. The benchmarking phase then executes the spec for a fixed number of iterations that roughly fit into 10 seconds. The actual time is recorded from start to finish, and the execution time per iteration is reported. If an assertion fails during this process, the benchmark will be marked as failed.



### Assertions

Lore currently provides the following **assertion functions** in the module `lore.test`:

- `assert(condition: Boolean): Unit` fails with a standard message if `condition` is false.
- `assert(condition: Boolean, message: String): Unit` fails with `message` if `condition` is false.
- `assert_equal(a: Any, b: Any): Unit` fails with a standard message if `a != b`.
- `assert_equal(a: Any, b: Any, message: String): Unit` fails with `message` if `a != b`.

These assertions are regular multi-functions and may be used anywhere. As Lore currently doesn't support exceptions, assertion failures are implemented using the VM's native exceptions. These cannot be caught by Lore code, but are instead handled by the VM. If a spec throws an assertion exception, it causes an individual test/benchmark to fail.

Assertions have an impact on benchmark performance. A spec that performs complex calculations, but is verified using simple assertions, should not have any measurable assertion overhead, and can thus act both as a benchmark and a test. However, if a small piece of code or simple function should be benchmarked, it might be useful to split a spec into a test and a benchmark, the latter marked as `@bench_only`. 

You can try benchmarking an assertion call such as `assert_equal(a, b)`, where `a` and `b` are values you'd typically compare, to gauge the assertion overhead for a given benchmark. A large portion of the overhead comes from the function call to `assert`/`assert_equal`, and the equality test in case of `assert_equal`.



### Motivation and Limitations

Not all programming languages provide a built-in testing framework. Languages that do include Rust, Elixir, D, and Crystal. There are arguments for and against this. From a language design perspective, it certainly isn't as clear-cut as, say, whether to implement a module/package/namespace system. In case of Lore, we believe that providing a simple base for testing and benchmarking is the way to go.

Here are some **advantages** that have motivated this decision:

- Tests that are part of the language are seamless to use for a veteran and easy to pick up for a newcomer. There is no need to include a third-party library, or to set up and maintain a testing toolchain.
- Instead of adding specs to separate test files and folders, they *can* be placed directly beside functions and types. This close relationship promotes writing tests and benchmarks, and makes them easier to find. The latter is important for specs that are suitable usage examples. They are only useful if they're easy to find.
- Lore itself needs a testing framework to execute language tests. Instead of building an internal solution that isn't suitable to use outside the language implementation, we've set out to build a solution that works as a base for everyone.
- Given that Lore doesn't have macros (yet), a comparable third-party testing framework would have to use a more verbose syntax for defining tests. Built-in tests can be implemented with custom syntax.
- As the language evolves, so can the test framework. In fact, it *must* evolve with the language, which forces us, the language designers, to take testing into account when we expand or change the language.

Now, all this isn't meant to exclude community testing or benchmarking frameworks. We want specs to work as a **baseline** that frameworks can build upon. This isn't currently the case, as there's no way to hook into specs, but it's a goal for the future. 

Given that the built-in testing solution should work as a baseline, we have to provide a conservative lowest common denominator. This comes with some limitations. In particular, Lore does not implement or otherwise prescribe object mocking, dependency injection, a specific DSL for testing, or any other features supported by more complex testing frameworks.
